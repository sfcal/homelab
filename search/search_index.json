{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homelab Infrastructure as Code","text":"<p>A complete infrastructure-as-code solution for managing a home Kubernetes cluster running on Proxmox VMs. This repository contains automation for the entire lifecycle - from VM template creation to Kubernetes application deployment.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Deploy a complete homelab environment with a single command:</p> <pre><code>apt update &amp;&amp; apt install -y curl &amp;&amp; bash -c \"$(curl -fsSL https://raw.githubusercontent.com/sfcal/homelab/refs/heads/main/deploy/controlplane.sh)\"\n</code></pre>"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>homelab/\n\u251c\u2500\u2500 ansible/           # Ansible playbooks for K3s deployment\n\u251c\u2500\u2500 ansible-runner/    # Docker container for running Ansible\n\u251c\u2500\u2500 deploy/            # Deployment scripts\n\u251c\u2500\u2500 docker-compose/    # Docker Compose stacks for standalone services\n\u251c\u2500\u2500 kubernetes/        # Kubernetes configurations (GitOps with FluxCD)\n\u251c\u2500\u2500 packer/            # Packer templates for Proxmox VMs\n\u2514\u2500\u2500 terraform/         # Terraform modules for infrastructure provisioning\n</code></pre>"},{"location":"#network-structure","title":"Network Structure","text":""},{"location":"#components","title":"Components","text":""},{"location":"#packer","title":"Packer","text":"<p>Automates the creation of VM templates in Proxmox with: - Ubuntu Server 24.04 (Noble) - Docker pre-installed - Cloud-init integration for dynamic provisioning - Optimized for Proxmox virtualization</p>"},{"location":"#terraform","title":"Terraform","text":"<p>Deploys infrastructure on Proxmox using the templates created by Packer: - Control plane VM - K3s master nodes cluster - K3s worker nodes - Modular approach for different environments (WIL, NYC)</p>"},{"location":"#ansible","title":"Ansible","text":"<p>Automates the deployment of K3s Kubernetes clusters: - High-availability control plane - Worker nodes - Load balancer configuration (MetalLB) - Network setup with kube-vip - Secure token-based authentication</p>"},{"location":"#kubernetes","title":"Kubernetes","text":"<p>GitOps-based Kubernetes configuration using FluxCD: - Core infrastructure services (cert-manager, Traefik, Longhorn) - Monitoring stack with Prometheus and Grafana - Application deployments with kustomize - Multi-environment support (WIL, NYC)</p>"},{"location":"#docker-compose-stacks","title":"Docker Compose Stacks","text":"<p>Standalone service stacks for specific use cases:</p> <p>Media Stack (docker-compose/media-stack) - Plex Media Server - Sonarr, Radarr for media management - SABnzbd, NZBHydra2 for downloading - Tdarr for transcoding - Ombi for media requests</p> <p>Monitoring Stack (docker-compose/monitoring-stack) - Prometheus for metrics collection - Grafana for visualization - InfluxDB and Telegraf for time-series data - Various exporters for specialized metrics</p>"},{"location":"#deployment-flow","title":"Deployment Flow","text":"<ol> <li>Template Creation: Packer builds Ubuntu VM templates in Proxmox</li> <li>Infrastructure Deployment: Terraform provisions VMs from templates</li> <li>Kubernetes Setup: Ansible configures K3s cluster on VMs</li> <li>Application Deployment: FluxCD deploys applications to Kubernetes</li> </ol>"},{"location":"#multi-environment-support","title":"Multi-Environment Support","text":"<p>This repository supports multiple environments with location-based naming:</p> <ul> <li>WIL: Development/testing environment</li> <li>NYC: Production environment</li> </ul> <p>Each environment can have its own configuration while sharing common base components.</p>"},{"location":"#running-with-ansible-runner","title":"Running with Ansible Runner","text":"<p>For consistent execution of Ansible playbooks, you can use the included Ansible Runner container:</p> <pre><code># Build the container\ncd ansible-runner\ndocker-compose build\n\n# Run the container\ndocker-compose up -d\n\n# Execute Ansible inside the container\ndocker exec -it ansible-runner ansible-playbook /runner/site.yml\n</code></pre>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Proxmox VE server (tested with 8.0+)</li> <li>SSH access to Proxmox</li> <li>Proxmox API token for automation</li> <li>Network with DHCP (or static IP configuration)</li> <li>Git for version control</li> <li>Internet access for downloading packages</li> </ul>"},{"location":"#maintenance-and-updates","title":"Maintenance and Updates","text":"<ul> <li>Templates: Rebuild Packer templates when OS updates are needed</li> <li>Infrastructure: Use Terraform to scale or modify VM resources</li> <li>Kubernetes: Update through GitOps with FluxCD</li> <li>Applications: Manage through Kubernetes manifests or Docker Compose for standalone services</li> </ul>"},{"location":"#other-resources","title":"Other Resources","text":"<ul> <li>Dotfiles - My personal configuration files</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"concepts/kubernetes/","title":"Kubernetes Configuration for Homelab","text":"<p>This directory contains the Kubernetes configuration for a GitOps-managed Kubernetes cluster using FluxCD. The infrastructure is designed to be declarative, versioned, and self-healing, with support for multiple environments (homelab and production).</p>"},{"location":"concepts/kubernetes/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Architecture Overview</li> <li>Directory Structure</li> <li>Key Components</li> <li>Deployment Flow</li> <li>Configuration Guide</li> <li>Adding Applications</li> <li>Multi-Environment Deployment</li> </ul>"},{"location":"concepts/kubernetes/#architecture-overview","title":"Architecture Overview","text":"<p>This Kubernetes configuration implements a GitOps approach using FluxCD, where:</p> <ol> <li>All configuration is stored in Git (this repository)</li> <li>FluxCD continuously monitors the repository for changes</li> <li>When changes are detected, they are automatically applied to the cluster</li> <li>The actual state of the cluster is reconciled with the desired state</li> </ol> <p>The setup includes a complete infrastructure layer (networking, storage, monitoring, certs) and an application layer, organized using Kustomize overlays.</p>"},{"location":"concepts/kubernetes/#directory-structure","title":"Directory Structure","text":"<pre><code>kubernetes/\n\u251c\u2500\u2500 .gitignore                 # Ignores sensitive files like secrets\n\u251c\u2500\u2500 apps/                      # Application deployments\n\u2502   \u251c\u2500\u2500 base/                  # Base application configurations\n\u2502   \u2502   \u2514\u2500\u2500 nginx/             # Example base application (nginx)\n\u2502   \u2514\u2500\u2500 homelab/               # Environment-specific application overlays\n\u2502       \u2514\u2500\u2500 nginx/             # Environment-specific nginx config\n\u251c\u2500\u2500 cluster/                   # Cluster-specific configurations\n\u2502   \u2514\u2500\u2500 homelab/               # Homelab cluster settings\n\u2502       \u251c\u2500\u2500 apps.yaml          # Application layer definition\n\u2502       \u251c\u2500\u2500 cluster-settings.yaml  # Cluster variables as ConfigMap\n\u2502       \u251c\u2500\u2500 flux-system-automation.yaml # Flux image automation\n\u2502       \u251c\u2500\u2500 infrastructure.yaml # Infrastructure layer definition\n\u2502       \u2514\u2500\u2500 kustomization.yaml  # Cluster kustomization\n\u251c\u2500\u2500 flux-system/               # Flux system configurations\n\u2502   \u251c\u2500\u2500 gotk-sync.yaml         # Git repository sync definition\n\u2502   \u2514\u2500\u2500 kustomization.yaml     # Flux system kustomization\n\u2514\u2500\u2500 infrastructure/            # Infrastructure components\n    \u251c\u2500\u2500 cert-manager/          # Certificate management\n    \u251c\u2500\u2500 external-secrets/      # External secrets management\n    \u251c\u2500\u2500 kube-prometheus-stack/ # Monitoring stack\n    \u251c\u2500\u2500 longhorn/              # Distributed storage\n    \u251c\u2500\u2500 sources/               # Helm repositories and Git sources\n    \u251c\u2500\u2500 traefik/               # Ingress controller\n    \u2514\u2500\u2500 kustomization.yaml     # Infrastructure kustomization\n</code></pre>"},{"location":"concepts/kubernetes/#key-components","title":"Key Components","text":""},{"location":"concepts/kubernetes/#tech-stack","title":"Tech stack","text":"Logo Name Description Ansible Automate bare metal provisioning and configuration ArgoCD GitOps tool built to deploy applications to Kubernetes cert-manager Cloud native certificate management Cilium eBPF-based Networking, Observability and Security (CNI, LB, Network Policy, etc.) Cloudflare DNS and Tunnel Docker Ephemeral PXE server ExternalDNS Synchronizes exposed Kubernetes Services and Ingresses with DNS providers Fedora Server Base OS for Kubernetes nodes Gitea Self-hosted Git service Grafana Observability platform Helm The package manager for Kubernetes K3s Lightweight distribution of Kubernetes Kanidm Modern and simple identity management platform Kubernetes Container-orchestration system, the backbone of this project Loki Log aggregation system NGINX Kubernetes Ingress Controller Nix Convenient development shell ntfy Notification service to send notifications to your phone or desktop Prometheus Systems monitoring and alerting toolkit Renovate Automatically update dependencies Rook Ceph Cloud-Native Storage for Kubernetes Tailscale VPN without port forwarding Wireguard Fast, modern, secure VPN tunnel Woodpecker CI Simple yet powerful CI/CD engine with great extensibility Zot Registry Private container registry"},{"location":"concepts/kubernetes/#infrastructure-layer","title":"Infrastructure Layer","text":"<ol> <li>Traefik (infrastructure/traefik/)</li> <li>Ingress controller for routing external traffic to services</li> <li>Configured with automatic HTTPS and middleware support</li> <li> <p>Includes security headers and dashboard access</p> </li> <li> <p>Cert-Manager (infrastructure/cert-manager/)</p> </li> <li>Manages TLS certificates automatically</li> <li>Configured with Let's Encrypt issuers (staging and production)</li> <li> <p>Uses DNS01 challenge with Cloudflare integration</p> </li> <li> <p>Longhorn (infrastructure/longhorn/)</p> </li> <li>Distributed block storage for persistent volumes</li> <li>Provides replicated storage across nodes</li> <li> <p>Includes UI for storage management</p> </li> <li> <p>Kube-Prometheus-Stack (infrastructure/kube-prometheus-stack/)</p> </li> <li>Comprehensive monitoring solution</li> <li>Includes Prometheus, Grafana, and Alertmanager</li> <li> <p>Pre-configured dashboards and alerts</p> </li> <li> <p>External-Secrets (infrastructure/external-secrets/)</p> </li> <li>Synchronizes secrets from external sources</li> <li>Configured with Kubernetes backend in this setup</li> <li>Enables secure secret management</li> </ol>"},{"location":"concepts/kubernetes/#application-layer","title":"Application Layer","text":"<p>The application layer is structured with:</p> <ol> <li>Base configurations (apps/base/)</li> <li>Contains the common configuration for applications</li> <li> <p>Example: nginx deployment and service definitions</p> </li> <li> <p>Environment overlays (apps/homelab/)</p> </li> <li>Environment-specific customizations</li> <li>Example: nginx ingress routes for specific domains</li> </ol>"},{"location":"concepts/kubernetes/#gitops-automation","title":"GitOps Automation","text":"<p>The GitOps pipeline is managed by FluxCD:</p> <ol> <li>Git Repository Source (flux-system/gotk-sync.yaml)</li> <li>Configures the Git repository to monitor</li> <li> <p>Set to sync from the main branch every minute</p> </li> <li> <p>Kustomizations (cluster/homelab/)</p> </li> <li>Defines what parts of the repo to deploy</li> <li>Sets dependencies between infrastructure and apps</li> </ol>"},{"location":"concepts/kubernetes/#deployment-flow","title":"Deployment Flow","text":"<p>The deployment follows this sequence:</p> <ol> <li>FluxCD is installed on the cluster and pointed to this repository</li> <li>FluxCD deploys the infrastructure components first</li> <li>Sources (Helm repositories)</li> <li>Cert-Manager</li> <li>Traefik</li> <li>External-Secrets</li> <li>Longhorn</li> <li>Kube-Prometheus-Stack</li> <li>Once infrastructure is ready, applications are deployed</li> <li>Currently configured: nginx example app</li> </ol>"},{"location":"concepts/kubernetes/#configuration-guide","title":"Configuration Guide","text":""},{"location":"concepts/kubernetes/#cluster-settings","title":"Cluster Settings","text":"<p>The file <code>cluster/homelab/cluster-settings.yaml</code> contains key cluster-wide settings:</p> <pre><code># Example settings\nTIMEZONE: \"America/Los_Angeles\"\nDOMAIN: \"local.samuelcalvert.com\"\nMETALLB_LB_RANGE: \"10.1.10.140-10.1.10.150\"\nCLUSTER_CIDR: \"10.42.0.0/16\"\nSERVICE_CIDR: \"10.43.0.0/16\"\n</code></pre> <p>Modify these values to match your environment.</p>"},{"location":"concepts/kubernetes/#tls-certificates","title":"TLS Certificates","text":"<p>TLS certificates are managed through cert-manager:</p> <ol> <li>Update the domain in <code>infrastructure/cert-manager/certificates/production/local-samuelcalvert-com.yaml</code></li> <li>Configure Cloudflare API token in <code>infrastructure/cert-manager/issuers/cloudflare-token-externalsecret.yaml</code></li> </ol>"},{"location":"concepts/kubernetes/#ingress-routes","title":"Ingress Routes","text":"<p>All ingress routes use the Traefik CRD format. Example from nginx:</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: nginx\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`nginx.local.samuelcalvert.com`)\n      kind: Rule\n      services:\n        - name: nginx\n          port: 80\n  tls:\n    secretName: local-samuelcalvert-com-tls\n</code></pre>"},{"location":"concepts/kubernetes/#adding-applications","title":"Adding Applications","text":"<p>To add a new application:</p> <ol> <li>Create a base configuration in <code>apps/base/&lt;app-name&gt;/</code>:</li> <li>deployment.yaml</li> <li>service.yaml</li> <li> <p>kustomization.yaml</p> </li> <li> <p>Create an environment overlay in <code>apps/homelab/&lt;app-name&gt;/</code>:</p> </li> <li>ingress.yaml (if needed)</li> <li> <p>kustomization.yaml (referencing the base)</p> </li> <li> <p>Update <code>apps/homelab/kustomization.yaml</code> to include your new application</p> </li> </ol>"},{"location":"concepts/kubernetes/#example-adding-a-new-app","title":"Example: Adding a New App","text":"<ol> <li>Create base files in <code>apps/base/myapp/</code>:</li> </ol> <pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:latest\n</code></pre> <pre><code># service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\n  namespace: default\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n    targetPort: 8080\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- deployment.yaml\n- service.yaml\n</code></pre> <ol> <li>Create overlay files in <code>apps/homelab/myapp/</code>:</li> </ol> <pre><code># ingress.yaml\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: myapp\n  namespace: default\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`myapp.local.samuelcalvert.com`)\n      kind: Rule\n      services:\n        - name: myapp\n          port: 80\n  tls:\n    secretName: local-samuelcalvert-com-tls\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- ../../base/myapp\n- ingress.yaml\n</code></pre> <ol> <li>Add to <code>apps/homelab/kustomization.yaml</code>:</li> </ol> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- nginx\n- myapp  # Add this line\n</code></pre> <p>Once committed to the repository, FluxCD will automatically deploy the new application.</p>"},{"location":"concepts/kubernetes/#multi-environment-deployment","title":"Multi-Environment Deployment","text":"<p>This configuration can be extended to support multiple environments, such as adding a production environment alongside the existing homelab setup. Here's how the directory structure would expand to accommodate a production environment:</p> <pre><code>kubernetes/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 apps/\n\u2502   \u251c\u2500\u2500 base/                  # Shared base configurations (unchanged)\n\u2502   \u2502   \u2514\u2500\u2500 nginx/\n\u2502   \u251c\u2500\u2500 homelab/               # Homelab environment overlays (unchanged)\n\u2502   \u2502   \u2514\u2500\u2500 nginx/\n\u2502   \u2514\u2500\u2500 production/            # New production environment overlays\n\u2502       \u251c\u2500\u2500 kustomization.yaml # References all production apps\n\u2502       \u251c\u2500\u2500 nginx/             # Production-specific nginx overlay\n\u2502       \u2502   \u251c\u2500\u2500 ingress.yaml   # Production ingress (example.com vs local domain)\n\u2502       \u2502   \u251c\u2500\u2500 kustomization.yaml # References base with production patches\n\u2502       \u2502   \u2514\u2500\u2500 deployment-patch.yaml # Patch for higher replica count/resources\n\u2502       \u2514\u2500\u2500 other-apps/        # Other production applications\n\u251c\u2500\u2500 cluster/\n\u2502   \u251c\u2500\u2500 homelab/               # Homelab cluster settings (unchanged)\n\u2502   \u2514\u2500\u2500 production/            # New production cluster settings\n\u2502       \u251c\u2500\u2500 apps.yaml          # Points to ./kubernetes/apps/production\n\u2502       \u251c\u2500\u2500 cluster-settings.yaml  # Production environment variables\n\u2502       \u251c\u2500\u2500 flux-system-automation.yaml # Production image automation\n\u2502       \u251c\u2500\u2500 infrastructure.yaml # Points to production infrastructure components\n\u2502       \u2514\u2500\u2500 kustomization.yaml  # Production cluster kustomization\n\u251c\u2500\u2500 flux-system/               # Remains largely unchanged\n\u2514\u2500\u2500 infrastructure/\n    \u251c\u2500\u2500 base/                  # New shared infrastructure base configurations\n    \u2502   \u251c\u2500\u2500 cert-manager/      # Base cert-manager configuration\n    \u2502   \u251c\u2500\u2500 external-secrets/  # Base external-secrets configuration\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 homelab/               # Homelab-specific infrastructure overlays\n    \u2502   \u251c\u2500\u2500 kustomization.yaml # References all homelab infrastructure\n    \u2502   \u251c\u2500\u2500 cert-manager/      # Homelab cert-manager overlay\n    \u2502   \u251c\u2500\u2500 traefik/           # Homelab traefik configuration\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 production/            # Production-specific infrastructure\n        \u251c\u2500\u2500 kustomization.yaml # References all production infrastructure\n        \u251c\u2500\u2500 cert-manager/      # Production cert-manager overlay\n        \u2502   \u251c\u2500\u2500 certificates/  # Production domain certificates\n        \u2502   \u2514\u2500\u2500 kustomization.yaml\n        \u251c\u2500\u2500 traefik/           # Production traefik configuration\n        \u2502   \u251c\u2500\u2500 middleware/    # Production-specific security policies\n        \u2502   \u2514\u2500\u2500 values.yaml    # Higher replica count, resources, etc.\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"concepts/kubernetes/#key-differences-in-production-environment","title":"Key Differences in Production Environment","text":"<p>The production environment would typically differ from homelab in several important ways:</p> <ol> <li> <p>Domain Names: Using your actual domain instead of local domains    <pre><code># Production ingress example\n- match: Host(`www.example.com`)\n  # Instead of Host(`www.nginx.local.samuelcalvert.com`)\n</code></pre></p> </li> <li> <p>Resource Requirements: Higher replica counts and resource allocations    <pre><code># Production deployment patches\nspec:\n  replicas: 3  # Instead of 1 in dev\n  resources:\n    requests:\n      memory: \"512Mi\"  # Higher than dev\n      cpu: \"250m\"      # Higher than dev\n</code></pre></p> </li> <li> <p>TLS Configuration: Using production certificates and stricter security    <pre><code># Production TLS configuration  \ntls:\n  secretName: example-com-tls  # Production certificate\n</code></pre></p> </li> <li> <p>Network Configuration: Different IP ranges and possibly external load balancers    <pre><code># In production cluster-settings.yaml\nMETALLB_LB_RANGE: \"10.1.20.140-10.1.20.150\"  # Production IP range\n</code></pre></p> </li> <li> <p>Monitoring and Alerts: More comprehensive monitoring with production alerts    <pre><code># In production prometheus values\nalertmanager:\n  receivers:\n    - name: 'production-team'\n      email_configs:\n        - to: 'oncall@example.com'\n</code></pre></p> </li> </ol>"},{"location":"concepts/kubernetes/#setting-up-a-production-environment","title":"Setting Up a Production Environment","text":"<p>To add a production environment:</p> <ol> <li>Create the directory structure shown above</li> <li>Configure the cluster resources in <code>cluster/production/</code></li> <li>Create production-specific application overlays in <code>apps/production/</code></li> <li>Set up production infrastructure configurations in <code>infrastructure/production/</code></li> <li>Optionally refactor shared configurations into <code>infrastructure/base/</code></li> </ol> <p>This structure provides a clean separation between environments while allowing you to reuse common configurations. The infrastructure is also refactored to have base components that can be customized per environment, giving you more flexibility in how you configure each environment.</p>"},{"location":"reference/components/ansible/","title":"Ansible Configuration for K3s Cluster Deployment","text":"<p>This directory contains Ansible playbooks and roles for automated deployment and management of a K3s Kubernetes cluster. The automation is designed to be flexible, idempotent, and supports both single-node and high-availability configurations.</p>"},{"location":"reference/components/ansible/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Architecture Overview</li> <li>Directory Structure</li> <li>Key Components</li> <li>Deployment Flow</li> <li>Configuration Guide</li> <li>Usage Examples</li> <li>Multi-Environment Deployment</li> </ul>"},{"location":"reference/components/ansible/#architecture-overview","title":"Architecture Overview","text":"<p>This Ansible configuration automates the deployment of a K3s Kubernetes cluster with the following features:</p> <ol> <li>High-availability control plane with multiple master nodes</li> <li>Worker node support for distributed workloads</li> <li>Virtual IP (kube-vip) for reliable API server access</li> <li>MetalLB for service load balancing</li> <li>Flannel for container networking</li> <li>Secure token-based cluster authentication</li> <li>Proper systemd service configuration</li> </ol> <p>The automation follows infrastructure-as-code best practices and is designed to be idempotent (can be run multiple times safely).</p>"},{"location":"reference/components/ansible/#directory-structure","title":"Directory Structure","text":"<pre><code>ansible/\n\u251c\u2500\u2500 .gitignore                 # Ignores kubeconfig file\n\u251c\u2500\u2500 ansible.cfg                # Ansible configuration\n\u251c\u2500\u2500 group_vars/                # Group variables\n\u2502   \u2514\u2500\u2500 all.yml                # Variables applied to all hosts\n\u251c\u2500\u2500 hosts.ini                  # Inventory file defining host groups\n\u251c\u2500\u2500 reset.yml                  # Playbook to reset/remove K3s\n\u251c\u2500\u2500 roles/                     # Role definitions\n\u2502   \u251c\u2500\u2500 download/              # Downloads K3s binaries\n\u2502   \u251c\u2500\u2500 k3s_agent/             # Configures K3s agent nodes\n\u2502   \u251c\u2500\u2500 k3s_server/            # Configures K3s server nodes\n\u2502   \u251c\u2500\u2500 k3s_server_post/       # Post-installation configuration\n\u2502   \u251c\u2500\u2500 prereq/                # Sets up system prerequisites\n\u2502   \u2514\u2500\u2500 reset/                 # Handles cluster reset tasks\n\u2514\u2500\u2500 site.yml                   # Main playbook for deployment\n</code></pre>"},{"location":"reference/components/ansible/#key-components","title":"Key Components","text":""},{"location":"reference/components/ansible/#playbooks","title":"Playbooks","text":"<ol> <li>site.yml</li> <li>The main playbook that orchestrates the entire K3s deployment</li> <li>Applies roles in the correct sequence to build the cluster</li> <li> <p>Handles both initial deployment and updates</p> </li> <li> <p>reset.yml</p> </li> <li>Removes K3s components and cleans up the system</li> <li>Useful for redeploying or decommissioning the cluster</li> </ol>"},{"location":"reference/components/ansible/#roles","title":"Roles","text":"<ol> <li>prereq</li> <li>Sets system timezone</li> <li>Enables IPv4/IPv6 forwarding</li> <li> <p>Configures system settings required for Kubernetes</p> </li> <li> <p>download</p> </li> <li>Downloads the correct K3s binary for the target architecture</li> <li> <p>Verifies checksums for security</p> </li> <li> <p>k3s_server</p> </li> <li>Installs and configures K3s server (master) nodes</li> <li>Sets up high-availability with kube-vip</li> <li>Configures token authentication</li> <li> <p>Creates kubeconfig for cluster access</p> </li> <li> <p>k3s_agent</p> </li> <li>Installs and configures K3s agent (worker) nodes</li> <li> <p>Joins nodes to the cluster using the secure token</p> </li> <li> <p>k3s_server_post</p> </li> <li>Configures MetalLB for service load balancing</li> <li> <p>Applies post-installation settings</p> </li> <li> <p>reset</p> </li> <li>Stops K3s services</li> <li>Unmounts filesystems</li> <li>Removes binaries, configuration, and data</li> </ol>"},{"location":"reference/components/ansible/#deployment-flow","title":"Deployment Flow","text":"<p>The deployment follows this sequence:</p> <ol> <li>Preparation</li> <li>System prerequisites are configured</li> <li> <p>K3s binaries are downloaded</p> </li> <li> <p>Server Deployment</p> </li> <li>First master initializes the cluster</li> <li>Additional masters join the cluster</li> <li>kube-vip is deployed for high-availability</li> <li> <p>MetalLB manifests are prepared</p> </li> <li> <p>Agent Deployment</p> </li> <li>Worker nodes are configured</li> <li> <p>Workers join the cluster using the secure token</p> </li> <li> <p>Post-Configuration</p> </li> <li>MetalLB is fully configured</li> <li>Load balancer IP ranges are set</li> <li>Kubeconfig is exported for cluster access</li> </ol>"},{"location":"reference/components/ansible/#configuration-guide","title":"Configuration Guide","text":""},{"location":"reference/components/ansible/#inventory-setup","title":"Inventory Setup","text":"<p>The <code>hosts.ini</code> file defines the cluster nodes:</p> <pre><code>[master]\n10.1.10.51    # Master node 1\n10.1.10.52    # Master node 2\n10.1.10.53    # Master node 3\n\n[node]\n10.1.10.41    # Worker node 1\n10.1.10.42    # Worker node 2\n\n[k3s_cluster:children]\nmaster\nnode\n</code></pre>"},{"location":"reference/components/ansible/#key-variables","title":"Key Variables","text":"<p>The <code>group_vars/all.yml</code> file contains important configuration options:</p> <pre><code># K3s version\nk3s_version: v1.30.2+k3s2\n\n# User for SSH access\nansible_user: sfcal\n\n# API server virtual IP address\napiserver_endpoint: 10.1.10.222\n\n# Network interface for flannel\nflannel_iface: eth0\n\n# Cluster authentication token\nk3s_token: some-SUPER-DEDEUPER-secret-password\n\n# MetalLB configuration\nmetal_lb_mode: layer2\nmetal_lb_ip_range: 10.1.10.140-10.1.10.150\n</code></pre>"},{"location":"reference/components/ansible/#security-considerations","title":"Security Considerations","text":"<ul> <li>The <code>k3s_token</code> should be changed to a secure random value</li> <li>SSH keys should be properly configured for <code>ansible_user</code></li> <li>Consider using Ansible Vault for sensitive variables</li> </ul>"},{"location":"reference/components/ansible/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/components/ansible/#standard-deployment","title":"Standard Deployment","text":"<p>To deploy the K3s cluster:</p> <pre><code># Verify the inventory\nansible-inventory --graph\n\n# Check connectivity\nansible all -m ping\n\n# Deploy the cluster\nansible-playbook site.yml\n</code></pre>"},{"location":"reference/components/ansible/#adding-nodes","title":"Adding Nodes","text":"<p>To add more nodes:</p> <ol> <li>Add the node IP addresses to <code>hosts.ini</code> under the appropriate group</li> <li>Run the playbook again:    <pre><code>ansible-playbook site.yml\n</code></pre></li> </ol>"},{"location":"reference/components/ansible/#resetremove-cluster","title":"Reset/Remove Cluster","text":"<p>To completely remove K3s from all nodes:</p> <pre><code>ansible-playbook reset.yml\n</code></pre>"},{"location":"reference/components/ansible/#multi-environment-deployment","title":"Multi-Environment Deployment","text":"<p>This configuration can be extended to support multiple environments, such as WIL and NYC locations, by creating environment-specific inventory and variable files.</p>"},{"location":"reference/components/ansible/#directory-structure-for-multi-environment","title":"Directory Structure for Multi-Environment","text":"<pre><code>ansible/\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 wil/\n\u2502   \u2502   \u251c\u2500\u2500 hosts.ini          # WIL-specific inventory\n\u2502   \u2502   \u2514\u2500\u2500 group_vars/        # WIL-specific variables\n\u2502   \u2502       \u2514\u2500\u2500 all.yml\n\u2502   \u2514\u2500\u2500 nyc/\n\u2502       \u251c\u2500\u2500 hosts.ini          # NYC-specific inventory\n\u2502       \u2514\u2500\u2500 group_vars/        # NYC-specific variables\n\u2502           \u2514\u2500\u2500 all.yml\n\u251c\u2500\u2500 roles/                     # Shared roles\n\u2514\u2500\u2500 site.yml                   # Main playbook\n</code></pre>"},{"location":"reference/components/ansible/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<p>Each environment would have customized variables:</p> <p>WIL Environment (environments/wil/group_vars/all.yml) <pre><code>apiserver_endpoint: 10.1.10.222\nmetal_lb_ip_range: 10.1.10.140-10.1.10.150\n</code></pre></p> <p>NYC Environment (environments/nyc/group_vars/all.yml) <pre><code>apiserver_endpoint: 10.1.20.222\nmetal_lb_ip_range: 10.1.20.140-10.1.20.150\n</code></pre></p>"},{"location":"reference/components/ansible/#deploying-to-a-specific-environment","title":"Deploying to a Specific Environment","text":"<pre><code># Deploy to WIL environment\nansible-playbook -i environments/wil/hosts.ini site.yml\n\n# Deploy to NYC environment\nansible-playbook -i environments/nyc/hosts.ini site.yml\n</code></pre> <p>This approach allows for consistent automation across multiple environments while maintaining environment-specific configurations.</p>"},{"location":"reference/components/packer/","title":"Packer Configuration for Proxmox Templates","text":"<p>This directory contains Packer configuration files for creating VM templates in Proxmox. These templates serve as the foundation for deploying Kubernetes clusters and other infrastructure components.</p>"},{"location":"reference/components/packer/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Directory Structure</li> <li>Template Features</li> <li>Build Process</li> <li>Configuration Guide</li> <li>Usage Examples</li> <li>Multi-Environment Considerations</li> </ul>"},{"location":"reference/components/packer/#overview","title":"Overview","text":"<p>The Packer configuration automates the creation of Ubuntu Server Noble (24.04) VM templates with the following features:</p> <ol> <li>Fully automated installation via cloud-init</li> <li>Pre-installed Docker engine</li> <li>Optimized for Proxmox virtualization</li> <li>Cloud-init integration for dynamic VM provisioning</li> <li>User account with sudo privileges</li> </ol> <p>These templates provide a consistent base for infrastructure deployment across environments, reducing configuration drift and enabling infrastructure-as-code practices.</p>"},{"location":"reference/components/packer/#directory-structure","title":"Directory Structure","text":"<pre><code>packer/\n\u251c\u2500\u2500 proxmox/\n\u2502   \u251c\u2500\u2500 .gitignore                       # Ignores credential files\n\u2502   \u251c\u2500\u2500 credentials.pkr.hcl.example      # Example credentials file (not provided in repo files)\n\u2502   \u2514\u2500\u2500 ubuntu-server-noble/             # Ubuntu 24.04 (Noble) template configuration\n\u2502       \u251c\u2500\u2500 .gitignore                   # Ignores downloaded ISO files\n\u2502       \u251c\u2500\u2500 files/                       # Configuration files\n\u2502       \u2502   \u2514\u2500\u2500 99-pve.cfg               # Cloud-init configuration for Proxmox\n\u2502       \u251c\u2500\u2500 http/                        # HTTP server content for automated installation\n\u2502       \u2502   \u251c\u2500\u2500 meta-data                # Cloud-init metadata (empty)\n\u2502       \u2502   \u2514\u2500\u2500 user-data                # Cloud-init user data for automated setup\n\u2502       \u2514\u2500\u2500 ubuntu-server-noble-docker.pkr.hcl # Main Packer template configuration\n</code></pre>"},{"location":"reference/components/packer/#template-features","title":"Template Features","text":"<p>The Ubuntu Server Noble template includes:</p>"},{"location":"reference/components/packer/#system-configuration","title":"System Configuration","text":"<ul> <li>Ubuntu 24.04 LTS base installation</li> <li>Minimal installation with only essential packages</li> <li>Configured for cloud-init compatibility with Proxmox</li> <li>American English locale with UTC timezone</li> <li>QEMU guest agent for improved VM management</li> </ul>"},{"location":"reference/components/packer/#container-support","title":"Container Support","text":"<ul> <li>Docker Engine pre-installed</li> <li>containerd runtime configured</li> <li>User added to docker group for non-root usage</li> </ul>"},{"location":"reference/components/packer/#security-settings","title":"Security Settings","text":"<ul> <li>SSH server installed and configured</li> <li>Root login disabled</li> <li>Sudo access for the primary user</li> <li>SSH key authentication ready</li> </ul>"},{"location":"reference/components/packer/#storage-optimization","title":"Storage Optimization","text":"<ul> <li>Direct disk layout for improved performance</li> <li>No swap partition for container workloads</li> <li>Raw disk format for efficiency in Proxmox</li> </ul>"},{"location":"reference/components/packer/#build-process","title":"Build Process","text":"<p>The template build follows this sequence:</p> <ol> <li>Preparation</li> <li>Packer connects to Proxmox API</li> <li>VM is created with specified configuration</li> <li> <p>Ubuntu Server ISO is attached</p> </li> <li> <p>Installation</p> </li> <li>Automated installation via cloud-init</li> <li>Basic system configuration applied</li> <li> <p>Packages installed</p> </li> <li> <p>Provisioning</p> </li> <li>Cloud-init integration configured</li> <li>Docker and dependencies installed</li> <li> <p>System optimized for template usage</p> </li> <li> <p>Finalization</p> </li> <li>SSH host keys removed (will be regenerated on clone)</li> <li>Machine ID cleared</li> <li>System cleaned and prepared for templating</li> </ol>"},{"location":"reference/components/packer/#configuration-guide","title":"Configuration Guide","text":""},{"location":"reference/components/packer/#credentials-setup","title":"Credentials Setup","text":"<p>Create a <code>credentials.pkr.hcl</code> file based on the required variables:</p> <pre><code>proxmox_api_url = \"https://your-proxmox-server:8006/api2/json\"\nproxmox_api_token_id = \"your-token-id\"\nproxmox_api_token_secret = \"your-token-secret\"\nssh_password = \"temporary-ssh-password\"\n</code></pre>"},{"location":"reference/components/packer/#template-customization","title":"Template Customization","text":"<p>Key parameters in <code>ubuntu-server-noble-docker.pkr.hcl</code>:</p> <ul> <li><code>node</code>: Target Proxmox node (currently \"pve-dev01\")</li> <li><code>vm_id</code>: VM ID in Proxmox (currently 9000)</li> <li><code>vm_name</code>: Template name (currently \"ubuntu-server-noble\")</li> <li><code>cores</code>, <code>memory</code>: VM resources</li> <li><code>ssh_username</code>: Username for SSH access (currently \"sfcal\")</li> </ul>"},{"location":"reference/components/packer/#user-data-configuration","title":"User Data Configuration","text":"<p>The <code>user-data</code> file contains cloud-init configuration:</p> <ul> <li>User creation with sudo access</li> <li>Password and SSH key settings</li> <li>Package installation</li> <li>Storage configuration</li> </ul>"},{"location":"reference/components/packer/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/components/packer/#building-the-template","title":"Building the Template","text":"<pre><code># Navigate to the template directory\ncd packer/proxmox/ubuntu-server-noble\n\n# Initialize Packer plugins\npacker init ubuntu-server-noble-docker.pkr.hcl\n\n# Validate the configuration\npacker validate -var-file=../credentials.pkr.hcl ubuntu-server-noble-docker.pkr.hcl\n\n# Build the template\npacker build -var-file=../credentials.pkr.hcl ubuntu-server-noble-docker.pkr.hcl\n</code></pre>"},{"location":"reference/components/packer/#customizing-the-build","title":"Customizing the Build","text":"<p>To customize the template for different requirements:</p> <pre><code># Build with specific variables\npacker build \\\n  -var=\"vm_name=ubuntu-server-noble-k8s\" \\\n  -var=\"cores=2\" \\\n  -var=\"memory=4096\" \\\n  -var-file=../credentials.pkr.hcl \\\n  ubuntu-server-noble-docker.pkr.hcl\n</code></pre>"},{"location":"reference/components/packer/#multi-environment-considerations","title":"Multi-Environment Considerations","text":"<p>For deploying across multiple environments (like WIL and NYC):</p>"},{"location":"reference/components/packer/#template-strategy","title":"Template Strategy","text":"<p>Consider these approaches for multi-environment templates:</p> <ol> <li>Shared Templates</li> <li>Build once, clone to multiple environments</li> <li>Ensure templates are generic enough for all environments</li> <li> <p>Use cloud-init for environment-specific configuration</p> </li> <li> <p>Environment-Specific Templates</p> </li> <li>Maintain separate template configurations for each environment</li> <li>Use variables files to define environment differences:</li> </ol> <pre><code>packer/\n\u251c\u2500\u2500 proxmox/\n\u2502   \u251c\u2500\u2500 environments/\n\u2502   \u2502   \u251c\u2500\u2500 wil/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 variables.pkr.hcl  # WIL-specific variables\n\u2502   \u2502   \u2514\u2500\u2500 nyc/\n\u2502   \u2502       \u2514\u2500\u2500 variables.pkr.hcl  # NYC-specific variables\n\u2502   \u2514\u2500\u2500 ubuntu-server-noble/\n\u2502       \u2514\u2500\u2500 ubuntu-server-noble-docker.pkr.hcl\n</code></pre>"},{"location":"reference/components/packer/#building-for-specific-environments","title":"Building for Specific Environments","text":"<pre><code># Build for WIL environment\npacker build \\\n  -var-file=../credentials.pkr.hcl \\\n  -var-file=../environments/wil/variables.pkr.hcl \\\n  ubuntu-server-noble-docker.pkr.hcl\n\n# Build for NYC environment\npacker build \\\n  -var-file=../credentials.pkr.hcl \\\n  -var-file=../environments/nyc/variables.pkr.hcl \\\n  ubuntu-server-noble-docker.pkr.hcl\n</code></pre> <p>This approach allows for consistent template creation across different environments while accommodating environment-specific requirements.</p>"},{"location":"reference/components/terraform/","title":"Terraform Infrastructure for Homelab","text":"<p>This directory contains the Terraform configuration files for deploying a K3s Kubernetes cluster on Proxmox VMs. The infrastructure is designed to be modular, reusable, and easily configurable.</p>"},{"location":"reference/components/terraform/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Directory Structure</li> <li>Modules</li> <li>proxmox-vm</li> <li>k3s-cluster</li> <li>Environments</li> <li>Development (dev)</li> <li>Getting Started</li> <li>Key Configuration Variables</li> <li>Cluster Architecture</li> <li>Outputs</li> <li>Customization</li> <li>Notes</li> </ul>"},{"location":"reference/components/terraform/#directory-structure","title":"Directory Structure","text":"<pre><code>terraform/\n\u251c\u2500\u2500 .gitignore                # Ignores state files, credentials, and other sensitive data\n\u251c\u2500\u2500 environments/             # Environment-specific configurations\n\u2502   \u251c\u2500\u2500 dev/                  # Development environment configuration\n\u2502   \u2502   \u251c\u2500\u2500 .gitignore        # Environment-specific files to ignore\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf        # Local state backend configuration\n\u2502   \u2502   \u251c\u2500\u2500 main.tf           # Main configuration for deploying infrastructure\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf        # Output definitions (IP addresses, summaries)\n\u2502   \u2502   \u251c\u2500\u2500 providers.tf      # Proxmox provider configuration\n\u2502   \u2502   \u251c\u2500\u2500 terraform.tfvars.example # Example variables template\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf      # Variable definitions for the environment\n\u2502   \u2502   \u2514\u2500\u2500 versions.tf       # Terraform and provider version constraints\n\u2502   \u2514\u2500\u2500 prod/                 # Production environment (placeholder for future use)\n\u2514\u2500\u2500 modules/                  # Reusable infrastructure components\n    \u251c\u2500\u2500 k3s-cluster/          # Kubernetes cluster deployment module\n    \u2502   \u251c\u2500\u2500 main.tf           # Creates master and worker node sets\n    \u2502   \u251c\u2500\u2500 outputs.tf        # Exposes cluster node information\n    \u2502   \u251c\u2500\u2500 variables.tf      # Configurable cluster parameters\n    \u2502   \u2514\u2500\u2500 versions.tf       # Required provider versions\n    \u2514\u2500\u2500 proxmox-vm/           # Proxmox virtual machine module\n        \u251c\u2500\u2500 main.tf           # VM creation and provisioning logic\n        \u251c\u2500\u2500 outputs.tf        # Exposes VM details\n        \u251c\u2500\u2500 variables.tf      # Configurable VM parameters\n        \u2514\u2500\u2500 versions.tf       # Required provider versions\n</code></pre>"},{"location":"reference/components/terraform/#modules","title":"Modules","text":""},{"location":"reference/components/terraform/#proxmox-vm","title":"proxmox-vm","text":"<p>A reusable module for creating and provisioning Proxmox VMs with consistent configuration.</p> <p>Features: - Full and linked cloning from templates - Configurable resources (CPU, memory, disk) - Static IP or DHCP networking - Cloud-init integration - Optional provisioning for:   - SSH key setup   - Git installation   - Terraform installation   - Git repository cloning</p>"},{"location":"reference/components/terraform/#k3s-cluster","title":"k3s-cluster","text":"<p>Creates a complete K3s cluster with configurable master and worker nodes.</p> <p>Features: - Configurable number of master and worker nodes - Separate resource configurations for masters and workers - Static IP addressing with configurable ranges - Uses the proxmox-vm module for VM creation</p>"},{"location":"reference/components/terraform/#module-hierarchy-and-relationships","title":"Module Hierarchy and Relationships","text":"<p>The configuration follows a hierarchical pattern:</p> <ol> <li>Environment Layer: The <code>dev</code> environment in <code>environments/dev/main.tf</code> defines the specific infrastructure needs</li> <li>Orchestration Layer: The <code>k3s-cluster</code> module in <code>modules/k3s-cluster/main.tf</code> manages the cluster composition</li> <li>Implementation Layer: The <code>proxmox-vm</code> module in <code>modules/proxmox-vm/main.tf</code> handles the actual VM creation</li> </ol> <p>This architecture follows infrastructure-as-code best practices by: - Separating environments (dev/prod) - Creating reusable components (modules) - Maintaining a clean separation of concerns - Providing flexible configuration options - Abstracting implementation details</p>"},{"location":"reference/components/terraform/#environments","title":"Environments","text":""},{"location":"reference/components/terraform/#development-dev","title":"Development (dev)","text":"<p>The development environment is fully configured and ready to deploy:</p> <ul> <li>Defines one control plane VM (for cluster management)</li> <li>Creates a 3-node K3s master cluster (for control plane redundancy)</li> <li>Deploys 2 K3s worker nodes (for workload execution)</li> <li>Configures the network (DHCP for control plane, static IPs for cluster nodes)</li> <li>Sets up SSH access with provided keys</li> </ul>"},{"location":"reference/components/terraform/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Setup credentials:    <pre><code>cd terraform/environments/dev\ncp terraform.tfvars.example terraform.tfvars\n# Edit terraform.tfvars with your Proxmox credentials and SSH keys\n</code></pre></p> </li> <li> <p>Initialize Terraform:    <pre><code>terraform init\n</code></pre></p> </li> <li> <p>Plan deployment:    <pre><code>terraform plan\n</code></pre></p> </li> <li> <p>Apply deployment:    <pre><code>terraform apply\n</code></pre></p> </li> </ol>"},{"location":"reference/components/terraform/#key-configuration-variables","title":"Key Configuration Variables","text":"<p>These are the essential variables you'll need to configure before deploying:</p>"},{"location":"reference/components/terraform/#required-variables","title":"Required Variables","text":"Variable Description Location <code>proxmox_api_url</code> Proxmox API URL (e.g., \"https://proxmox.example.com:8006/api2/json\") terraform.tfvars <code>proxmox_api_token_id</code> Proxmox API token ID (e.g., \"terraform@pam!token\") terraform.tfvars <code>proxmox_api_token_secret</code> Proxmox API token secret terraform.tfvars <code>ssh_public_key</code> SSH public key for VM access terraform.tfvars <code>ssh_private_key_path</code> Path to SSH private key for provisioning terraform.tfvars"},{"location":"reference/components/terraform/#commonly-adjusted-settings","title":"Commonly Adjusted Settings","text":"Setting Description Default File Location Master node count Number of K3s master nodes 3 environments/dev/main.tf Worker node count Number of K3s worker nodes 2 environments/dev/main.tf VM resources CPU, memory, and disk settings Various environments/dev/main.tf Network settings IP addressing scheme 10.1.10.x environments/dev/main.tf <p>A full list of all configuration variables is available in the respective module directories: - Proxmox VM Module Variables - K3s Cluster Module Variables</p>"},{"location":"reference/components/terraform/#cluster-architecture","title":"Cluster Architecture","text":"<p>The default deployment creates: - Control Plane: 1 VM with 2 cores, 4GB RAM - K3s Masters: 3 VMs with 2 cores, 4GB RAM each - K3s Workers: 2 VMs with 2 cores, 4GB RAM each</p> <p>Network configuration: - Masters: 10.1.10.51-53 - Workers: 10.1.10.41-42 - Control Plane: DHCP</p>"},{"location":"reference/components/terraform/#prerequisites","title":"Prerequisites","text":"<ul> <li>Proxmox VE server</li> <li>Proxmox API token with appropriate permissions</li> <li>Ubuntu Server Noble (24.04) VM template named \"ubuntu-server-noble\"</li> <li>SSH keypair for VM access and provisioning</li> </ul>"},{"location":"reference/components/terraform/#outputs","title":"Outputs","text":"<p>After deployment, Terraform will output: - IP addresses of the control plane VM - IP addresses of all K3s master nodes - IP addresses of all K3s worker nodes - Complete infrastructure summary</p>"},{"location":"reference/components/terraform/#customization","title":"Customization","text":"<p>The most common customizations can be made by editing just a few files:</p> <ol> <li>Basic Settings: Edit <code>terraform.tfvars</code> in your environment directory</li> <li>Cluster Configuration: Modify <code>main.tf</code> in your environment directory</li> <li>Advanced Settings: Update module parameters in <code>main.tf</code></li> </ol>"},{"location":"reference/components/terraform/#examples","title":"Examples","text":"<p>Adjusting Cluster Size</p> <p>In <code>environments/dev/main.tf</code>: <pre><code>module \"k3s_cluster\" {\n  source = \"../../modules/k3s-cluster\"\n\n  master_count = 5    # Increase from 3 to 5 masters\n  worker_count = 10   # Increase from 2 to 10 workers\n}\n</code></pre></p> <p>Customizing VM Resources</p> <p>In <code>environments/dev/main.tf</code>: <pre><code>module \"k3s_cluster\" {\n  source = \"../../modules/k3s-cluster\"\n\n  master_memory = 8192  # 8GB RAM for masters\n  worker_memory = 16384 # 16GB RAM for workers\n  master_cores  = 4     # 4 CPU cores for masters\n  worker_cores  = 8     # 8 CPU cores for workers\n}\n</code></pre></p>"},{"location":"reference/components/terraform/#notes","title":"Notes","text":"<ul> <li>The Terraform configuration uses the telmate/proxmox provider</li> <li>VM provisioning is optional and can be enabled/disabled as needed</li> <li>The template assumes Ubuntu-based VMs with cloud-init support</li> <li>For HA K3s clusters, a minimum of 3 master nodes is recommended</li> <li>The configuration can be extended to support multiple environments (dev, staging, prod)</li> <li>The modules can be reused across different projects</li> </ul>"}]}